{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup for the problem decomposition project; API keys and dataset\n",
    "import os, getpass\n",
    "from utils import set_env\n",
    "from datasets import load_dataset\n",
    "\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "set_env(\"LANGCHAIN_API_KEY\")\n",
    "set_env('LANGCHAIN_TRACING_V2')\n",
    "set_env('LANGCHAIN_ENDPOINT') \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"problem_decomposition\" # Langsmith project name\n",
    "\n",
    "# Load the dataset\n",
    "dev_set = load_dataset(\"hotpot_qa\", 'distractor', split = 'validation', trust_remote_code = True)\n",
    "test_set = dev_set.shuffle(seed = 42) # Use as test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem decomposition experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem_decomposition import build_graph\n",
    "from utils import extract_data_info\n",
    "\n",
    "# Load Graph\n",
    "# ExperimentName = 'gpt3.5_turbo_decomposition' \n",
    "ExperimentName = 'demo'\n",
    "graph = build_graph('openai', ExperimentName)\n",
    "\n",
    "# Run\n",
    "iteration = 1\n",
    "\n",
    "for i in range(iteration):\n",
    "    data = test_set[i]\n",
    "\n",
    "    # Extract data info\n",
    "    unique_id, question, context = extract_data_info(data)\n",
    "\n",
    "    # Invoke the graphs\n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    prompt = {\n",
    "        'messages': question,\n",
    "        'context': context\n",
    "    }\n",
    "    response = graph.invoke(prompt, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses (states) into json format\n",
    "import json\n",
    "# instantiate states\n",
    "states = {\n",
    "    'id': [],\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'gt_answer': [],\n",
    "    'supporting_documents': [],\n",
    "    'supporting_facts': [],\n",
    "    'problem': [],\n",
    "    'sub_problems': [],\n",
    "    'dependencies': [],\n",
    "    'sub_problem_solutions': [],\n",
    "    'sub_problem_reasoning': [],\n",
    "    'final_reasoning': [],\n",
    "    'context': []\n",
    "}\n",
    "\n",
    "# Get responses\n",
    "for i in range(iteration):\n",
    "    # Extract data info\n",
    "    unique_id = test_set[i]['id']\n",
    "    gt_answer = test_set[i]['answer']\n",
    "    supp_facts = test_set[i]['supporting_facts']['title']\n",
    "    supp_facts_idx = sorted(list(set(\n",
    "        [\n",
    "            test_set[i]['context']['title'].index(supp_fact_i) + 1\n",
    "            for supp_fact_i in supp_facts\n",
    "        ]\n",
    "    )))\n",
    "    \n",
    "    # load response \n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    curr_state = graph.get_state(thread).values\n",
    "    curr_state['supporting_documents'] = sorted(list(set(curr_state['supporting_documents']))) # convert to set\n",
    "\n",
    "    # Save state\n",
    "    for j in states.keys():\n",
    "        if j in curr_state.keys():\n",
    "            states[j].append(curr_state[j])\n",
    "    states['question'].append(curr_state['messages'][0].content)\n",
    "    states['answer'].append(curr_state['messages'][1].content)\n",
    "    states['gt_answer'].append(gt_answer)\n",
    "    states['supporting_facts'].append(supp_facts_idx)\n",
    "    states['id'].append(unique_id)\n",
    "\n",
    "json.dump(states, open(f'states/states_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-step LLM experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive import naive_decomposition, load_naive_prompt\n",
    "from utils import extract_data_info\n",
    "\n",
    "# settings\n",
    "ExperimentName = 'gpt3.5_turbo_naive'\n",
    "naive_llm = naive_decomposition(config.model)\n",
    "\n",
    "# Run\n",
    "iteration = 1000\n",
    "responses = {'id': [], 'response': []}\n",
    "for i in range(iteration):\n",
    "    data = test_set[i]\n",
    "\n",
    "    # Extract data info\n",
    "    unique_id, question, context = extract_data_info(data)\n",
    "\n",
    "    # Invoke the naive llm\n",
    "    prompt = load_naive_prompt(\n",
    "        question = question,\n",
    "        context = context\n",
    "    )\n",
    "    response = naive_llm.invoke(prompt)\n",
    "    responses['response'].append(response)\n",
    "    responses['id'].append(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save responses (states) into json format\n",
    "import json\n",
    "# instantiate states\n",
    "states = {\n",
    "    'id': [],\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'gt_answer': [],\n",
    "    'supporting_documents': [],\n",
    "    'supporting_facts': [],\n",
    "    'context': []\n",
    "}\n",
    "\n",
    "# Get responses\n",
    "for i in range(iteration):\n",
    "    # Check if the unique_id is the same\n",
    "    unique_id = test_set[i]['id']\n",
    "    assert unique_id == responses['id'][i]\n",
    "\n",
    "    # Extract data info\n",
    "    question = test_set[i]['question']\n",
    "    gt_answer = test_set[i]['answer']\n",
    "    supp_facts = test_set[i]['supporting_facts']['title']\n",
    "    supp_facts_idx = sorted(list(set(\n",
    "        [\n",
    "            test_set[i]['context']['title'].index(supp_fact_i) + 1\n",
    "            for supp_fact_i in supp_facts\n",
    "        ]\n",
    "    )))\n",
    "\n",
    "    # Extract response\n",
    "    naive_answer = responses['response'][i].answer\n",
    "    naive_supporting_docs = sorted(list(set(responses['response'][i].supporting_documents)))\n",
    "\n",
    "    # Save state\n",
    "    states['answer'].append(naive_answer)\n",
    "    states['supporting_documents'].append(naive_supporting_docs)\n",
    "    states['question'].append(question)\n",
    "    states['gt_answer'].append(gt_answer)\n",
    "    states['supporting_facts'].append(supp_facts_idx)\n",
    "    states['id'].append(unique_id)\n",
    "\n",
    "json.dump(states, open(f'states/states_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import HotpotEvaluator\n",
    "\n",
    "# Instantiate evaluator\n",
    "evaluator = HotpotEvaluator(openai_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Load states\n",
    "states = json.load(open(f'states/states_{ExperimentName}.json', 'r'))\n",
    "\n",
    "# Calculate metrics\n",
    "results = evaluator.calculate_metrics_answer(states)\n",
    "results_docs = evaluator.calculate_metrics_supporting_docs(states)\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(f'metrics/metrics_answer_{ExperimentName}.json', 'w'), indent = 4)\n",
    "json.dump(results_docs, open(f'metrics/metrics_docs_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main problem decomposition\n",
    "results_docs['avg_f1'], results_docs['exact_match_ratio'], results['avg_f1'], results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive main\n",
    "results_docs['avg_f1'], results_docs['exact_match_ratio'], results['avg_f1'], results['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results1, results2, results_docs1, results_docs2, name1=\"Model 1\", name2=\"Model 2\"):\n",
    "    # Choose color combination\n",
    "    color1, color2 = '#4A6274', '#94ACBF'    # Soft Blue & Sage Green\n",
    "    \n",
    "    # Create figure with horizontal subplot arrangement\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Shared settings\n",
    "    y_max = 1.0\n",
    "    label_size = 12\n",
    "    title_size = 14\n",
    "    tick_size = 10\n",
    "    width = 0.35\n",
    "    \n",
    "    # Plot 1: Main metrics\n",
    "    metrics = ['accuracy', 'exact_match_ratio', 'avg_precision', 'avg_recall', 'avg_f1']\n",
    "    metric_labels = ['ACC', 'EM', 'Precision', 'Recall', 'F1']\n",
    "    x = np.arange(len(metrics))\n",
    "    \n",
    "    # Create bars and add value labels\n",
    "    values1 = [results1[m] for m in metrics]\n",
    "    values2 = [results2[m] for m in metrics]\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, values1, width, color=color1)\n",
    "    bars2 = ax1.bar(x + width/2, values2, width, color=color2)\n",
    "    \n",
    "    # Add value labels above bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax1.set_ylabel('Score', fontsize=label_size)\n",
    "    ax1.set_ylim(0, y_max + 0.1)  # Add padding for labels\n",
    "    ax1.set_title('Performance Metrics', fontsize=title_size, pad=15)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(metric_labels, fontsize=tick_size)\n",
    "    ax1.tick_params(axis='y', labelsize=tick_size)\n",
    "    \n",
    "    # Plot 2: Documents metrics\n",
    "    doc_metrics = ['exact_match_ratio', 'avg_precision', 'avg_recall', 'avg_f1']\n",
    "    doc_labels = ['EM', 'Precision', 'Recall', 'F1']\n",
    "    x2 = np.arange(len(doc_metrics))\n",
    "    \n",
    "    # Create bars and add value labels\n",
    "    values3 = [results_docs1[m] for m in doc_metrics]\n",
    "    values4 = [results_docs2[m] for m in doc_metrics]\n",
    "    \n",
    "    bars3 = ax2.bar(x2 - width/2, values3, width, color=color1)\n",
    "    bars4 = ax2.bar(x2 + width/2, values4, width, color=color2)\n",
    "    \n",
    "    # Add value labels above bars\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    for bar in bars4:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax2.set_ylabel('Score', fontsize=label_size)\n",
    "    ax2.set_ylim(0, y_max + 0.1)  # Add padding for labels\n",
    "    ax2.set_title('Supporting Documents Metrics', fontsize=title_size, pad=15)\n",
    "    ax2.set_xticks(x2)\n",
    "    ax2.set_xticklabels(doc_labels, fontsize=tick_size)\n",
    "    ax2.tick_params(axis='y', labelsize=tick_size)\n",
    "    \n",
    "    # Add frameless legend to the figure\n",
    "    fig.legend([bars1, bars2], [name1, name2], \n",
    "              loc='right',\n",
    "              bbox_to_anchor=(0.18, 0.98),\n",
    "              frameon=False,\n",
    "              fontsize=tick_size)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    results1, results2, results_docs1, results_docs2, \n",
    "    name1=\"Decomposition\", name2=\"Naive\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal-directedness-7eAwMRxE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
