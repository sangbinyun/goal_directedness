{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env('LANGCHAIN_TRACING_V2')\n",
    "_set_env('LANGCHAIN_ENDPOINT')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"problem_decomposition\"\n",
    "\n",
    "# Test config file\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type = str, default = 'openai')\n",
    "\n",
    "config = parser.parse_args(args=['--model', 'openai'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from graph_builder import build_graph\n",
    "\n",
    "# Load Graph\n",
    "ExperimentName = 'gpt3.5_turbo_test1' # external memory\n",
    "graph = build_graph(config.model, ExperimentName)\n",
    "\n",
    "# Prep data\n",
    "# train_set = load_dataset(\"hotpot_qa\", 'distractor', split='train', trust_remote_code=True)\n",
    "dev_set = load_dataset(\"hotpot_qa\", 'distractor', split = 'validation', trust_remote_code = True)\n",
    "test_set = dev_set.shuffle(seed = 42) # Use as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "iteration = 100\n",
    "\n",
    "for i in range(iteration):\n",
    "    data = test_set[i]\n",
    "\n",
    "    # Extract data info\n",
    "    unique_id = data['id']\n",
    "    question = data['question']\n",
    "    context = [\n",
    "        \"{}: {}\".format(title, \" \".join(sentences)) \n",
    "        for title, sentences in zip(data['context']['title'], data['context']['sentences'])\n",
    "    ]\n",
    "\n",
    "    # Invoke the graphs\n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    prompt = {\n",
    "        'messages': question,\n",
    "        'context': context\n",
    "    }\n",
    "    response = graph.invoke(prompt, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states\n",
    "# instantiate states\n",
    "states = {\n",
    "    'id': [],\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'gt_answer': [],\n",
    "    'problem': [],\n",
    "    'sub_problems': [],\n",
    "    'dependencies': [],\n",
    "    'sub_problem_solutions': [],\n",
    "    'sub_problem_reasoning': [],\n",
    "    'final_reasoning': [],\n",
    "    'context': []\n",
    "}\n",
    "\n",
    "# Get responses\n",
    "for i in range(iteration):\n",
    "    # Extract data info\n",
    "    unique_id = test_set[i]['id']\n",
    "    gt_answer = test_set[i]['answer']\n",
    "    \n",
    "    # load response \n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    curr_state = graph.get_state(thread).values\n",
    "\n",
    "    # Save state\n",
    "    for j in states.keys():\n",
    "        if j in curr_state.keys():\n",
    "            states[j].append(curr_state[j])\n",
    "    states['question'].append(curr_state['messages'][0].content)\n",
    "    states['answer'].append(curr_state['messages'][1].content)\n",
    "    states['gt_answer'].append(gt_answer)\n",
    "    states['id'].append(unique_id)\n",
    "\n",
    "json.dump(states, open(f'states_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct': 3, 'incorrect': 2, 'accuracy': 0.6}\n"
     ]
    }
   ],
   "source": [
    "# llm based evaluation\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from prompt import PromptTemplate\n",
    "\n",
    "class HotpotEvaluator:\n",
    "    def __init__(self, openai_key: str):\n",
    "        \"\"\"Initialize evaluator with OpenAI key.\"\"\"\n",
    "        self.client = OpenAI(api_key=openai_key)\n",
    "        self.prompt_template = PromptTemplate.hotpot_evaluation\n",
    "        \n",
    "        self.yes_count = 0\n",
    "        self.no_count = 0\n",
    "        self.total = 0\n",
    "        self.correctness = {'id': [], 'list': []}\n",
    "\n",
    "    def evaluate_single(self, question: str, correct_answer: str, model_answer: str) -> str:\n",
    "        \"\"\"Evaluate a single answer using GPT-4o.\"\"\"\n",
    "        prompt = self.prompt_template.format(\n",
    "            question = question,\n",
    "            correct_answer = correct_answer,\n",
    "            answer = model_answer\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Evaluation failed: {e}\")\n",
    "            return \"ERROR\"\n",
    "\n",
    "    def evaluate_dataset(self, data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a dataset of hotpot QA pairs.\n",
    "        \n",
    "        Args:\n",
    "            data: Dict with keys 'question', 'gt_answer' (correct), and 'answer' (model response) and values as lists\n",
    "        \n",
    "        Returns:\n",
    "            Dict with evaluation metrics\n",
    "        \"\"\"\n",
    "        self.total = len(data['id'])\n",
    "\n",
    "        for i in range(self.total):\n",
    "            result = self.evaluate_single(\n",
    "                data['question'][i],\n",
    "                data['gt_answer'][i],\n",
    "                data['answer'][i]\n",
    "            )\n",
    "            \n",
    "            if 'yes' in result.lower():\n",
    "                self.yes_count += 1                \n",
    "                self.correctness['list'].append('yes')\n",
    "            elif 'no' in result.lower():\n",
    "                self.no_count += 1\n",
    "                self.correctness['list'].append('no')\n",
    "            self.correctness['id'].append(data['id'][i])\n",
    "\n",
    "        return {\n",
    "            'correct': self.yes_count,\n",
    "            'incorrect': self.no_count,\n",
    "            'accuracy': self.yes_count / self.total if self.total > 0 else 0,\n",
    "            'correctness': self.correctness\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "evaluator = HotpotEvaluator(openai_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "states = json.load(open('states_{ExperimentName}.json', 'r'))\n",
    "results = evaluator.evaluate_dataset(states)\n",
    "print(results)  # {'correct': 1, 'incorrect': 0, 'accuracy': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal-directedness-7eAwMRxE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
