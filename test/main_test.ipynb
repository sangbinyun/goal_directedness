{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env('LANGCHAIN_TRACING_V2')\n",
    "_set_env('LANGCHAIN_ENDPOINT')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"problem_decomposition\"\n",
    "\n",
    "# Test config file\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type = str, default = 'openai')\n",
    "\n",
    "config = parser.parse_args(args=['--model', 'openai'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from problem_decomposition import build_graph\n",
    "\n",
    "# Load Graph\n",
    "# ExperimentName = 'gpt3.5_turbo_test1' # external memory\n",
    "ExperimentName = 'prompt_3'\n",
    "graph = build_graph(config.model, ExperimentName)\n",
    "\n",
    "# Prep data\n",
    "# train_set = load_dataset(\"hotpot_qa\", 'distractor', split='train', trust_remote_code=True)\n",
    "dev_set = load_dataset(\"hotpot_qa\", 'distractor', split = 'validation', trust_remote_code = True)\n",
    "test_set = dev_set.shuffle(seed = 42) # Use as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "iteration = 100\n",
    "\n",
    "for i in range(13, iteration):\n",
    "    data = test_set[i]\n",
    "\n",
    "    # Extract data info\n",
    "    unique_id = data['id']\n",
    "    question = data['question']\n",
    "    context = [\n",
    "        \"{}. {}: {}\".format(doc_id+1, title, \" \".join(sentences)) \n",
    "        for doc_id, (title, sentences)\n",
    "        in enumerate(\n",
    "            zip(\n",
    "                data['context']['title'], \n",
    "                data['context']['sentences']\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Invoke the graphs\n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    prompt = {\n",
    "        'messages': question,\n",
    "        'context': context\n",
    "    }\n",
    "    response = graph.invoke(prompt, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states\n",
    "import json\n",
    "# instantiate states\n",
    "states = {\n",
    "    'id': [],\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'gt_answer': [],\n",
    "    'supporting_documents': [],\n",
    "    'supporting_facts': [],\n",
    "    'problem': [],\n",
    "    'sub_problems': [],\n",
    "    'dependencies': [],\n",
    "    'sub_problem_solutions': [],\n",
    "    'sub_problem_reasoning': [],\n",
    "    'final_reasoning': [],\n",
    "    'context': []\n",
    "}\n",
    "\n",
    "# Get responses\n",
    "for i in range(iteration):\n",
    "    # Extract data info\n",
    "    unique_id = test_set[i]['id']\n",
    "    gt_answer = test_set[i]['answer']\n",
    "    supp_facts = test_set[i]['supporting_facts']['title']\n",
    "    supp_facts_idx = sorted(list(set(\n",
    "        [\n",
    "            test_set[i]['context']['title'].index(supp_fact_i) + 1\n",
    "            for supp_fact_i in supp_facts\n",
    "        ]\n",
    "    )))\n",
    "    \n",
    "    # load response \n",
    "    thread = {\"configurable\": {\"thread_id\": unique_id}}\n",
    "    curr_state = graph.get_state(thread).values\n",
    "    curr_state['supporting_documents'] = sorted(list(set(curr_state['supporting_documents']))) # convert to set\n",
    "\n",
    "    # Save state\n",
    "    for j in states.keys():\n",
    "        if j in curr_state.keys():\n",
    "            states[j].append(curr_state[j])\n",
    "    states['question'].append(curr_state['messages'][0].content)\n",
    "    states['answer'].append(curr_state['messages'][1].content)\n",
    "    states['gt_answer'].append(gt_answer)\n",
    "    states['supporting_facts'].append(supp_facts_idx)\n",
    "    states['id'].append(unique_id)\n",
    "\n",
    "json.dump(states, open(f'states/states_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluator import HotpotEvaluator\n",
    "\n",
    "# Instantiate evaluator\n",
    "evaluator = HotpotEvaluator(openai_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Load states\n",
    "states = json.load(open(f'states/states_{ExperimentName}.json', 'r'))\n",
    "\n",
    "# Calculate metrics\n",
    "results = evaluator.calculate_metrics_answer(states)\n",
    "results_docs = evaluator.calculate_metrics_supporting_docs(states)\n",
    "\n",
    "# Save results\n",
    "json.dump(results, open(f'metrics/metrics_answer_{ExperimentName}.json', 'w'), indent = 4)\n",
    "json.dump(results_docs, open(f'metrics/metrics_docs_{ExperimentName}.json', 'w'), indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7557142857142856, 0.45, 0.5159900344715932, 0.75)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt 3!\n",
    "results_docs['avg_f1'], results_docs['exact_match_ratio'], results['avg_f1'], results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prussian ,  Otto von Bismarck\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"Otto von Bismarck\" refers to a character's name, not the nationality. The question asks for the nationality, which is \"Prussian.\" Therefore, the provided answer is not factually correct or complete., f1: 0, em: False\n",
      "\n",
      "Kurt Julian Weill ,  Kurt Weill\n",
      "corr: yes, res: YES, f1: 0.8, em: False\n",
      "\n",
      "U2 ,  U2 released the song 'With or Without You' first.\n",
      "corr: yes, res: YES, f1: 0.2222222222222222, em: False\n",
      "\n",
      "Oldham County ,  Oldham County, Kentucky\n",
      "corr: yes, res: YES, f1: 0.8, em: False\n",
      "\n",
      "138,535 ,  138535\n",
      "corr: yes, res: YES\n",
      "\n",
      "The answer \"138535\" is factually correct and complete as it matches the correct answer \"138,535\" in terms of numerical value, despite the lack of a comma., f1: 1.0, em: True\n",
      "\n",
      "1959 ,  1959\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "October 25, 1881 ,  Thomas Fitch defended John Henry Holliday after the gunfight at the O.K. Corral in 1881.\n",
      "corr: no, res: NO\n",
      "\n",
      "The evaluated answer is incomplete because it does not specify the exact date of the gunfight, which is October 25, 1881., f1: 0.125, em: False\n",
      "\n",
      "Vaisakhi List ,  Vaisakhi List\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Charlie Murphy ,  Charlie Murphy\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Dudman ,  Bill Dudman served as the Industrial Chaplain to the Bishop of Lincoln after his curacy at Frodingham, a suburb of Scunthorpe.\n",
      "corr: yes, res: YES, f1: 0.10526315789473684, em: False\n",
      "\n",
      "The Hungry ,  Como Ama\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"Como Ama\" is incorrect. The first two words of the fifth studio album of Joseph Edgar Foreman, also known as Afroman, are \"The Hungry.\", f1: 0, em: False\n",
      "\n",
      "Joint Chiefs of Staff ,  General Omar Bradley\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"General Omar Bradley\" is not fully correct. The question asks for the position that the Twelfth United States Army Group commander was the first chairman of, which is the \"Joint Chiefs of Staff.\" The provided answer gives the name of the person, not the position., f1: 0, em: False\n",
      "\n",
      "Rensselaer County ,  Lyman Sherwood was born in Rensselaer County, New York.\n",
      "corr: yes, res: YES, f1: 0.3636363636363636, em: False\n",
      "\n",
      "Harry F. Sinclair ,  Robert Earl Holding owned an oil company that was originally founded by who?\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer provided is a repetition of the question and does not provide the factual information that the oil company was originally founded by Harry F. Sinclair., f1: 0, em: False\n",
      "\n",
      "Axl Rose ,  Axl Rose\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "On March 27, 1977, two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los Rodeos Airport (now Tenerife North Airport) ,  Tenerife airport disaster occurred first.\n",
      "corr: yes, res: YES, f1: 0.12121212121212122, em: False\n",
      "\n",
      "no ,  No\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Nassau County ,  Nassau County\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "27 January 1974 ,  Ole Einar Bj√∏rndalen was born on January 27, 1974.\n",
      "corr: yes, res: YES, f1: 0.5, em: False\n",
      "\n",
      "Gasherbrum II ,  Gasherbrum II\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "John de Mol Jr. ,  John de Mol Jr.\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "1974 ,  The car depicted on the cover of the album 'Pentastar: In the Style of Demons' ceased production in 1974.\n",
      "corr: yes, res: YES, f1: 0.125, em: False\n",
      "\n",
      "Rachel Anne Maddow ,  Rachel Maddow and Michael Pollan\n",
      "corr: no, res: NO\n",
      "\n",
      "Explanation: The correct answer is Rachel Anne Maddow. The evaluated answer includes both Rachel Maddow and Michael Pollan, but only Rachel Maddow is correct. Therefore, the answer is not fully correct., f1: 0.5, em: False\n",
      "\n",
      "yes ,  Robert Wise had more award nominations than Zoltan Korda\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "1985 ,  1985\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "2009 ,  The English local newspaper, featuring the sculpture and war memorial in the Forbury Gardens, changed names in 2009.\n",
      "corr: yes, res: YES, f1: 0.125, em: False\n",
      "\n",
      "1970s and 1980s ,  Jose Gonzalo Rodriguez Gacha and other leaders of the Medellin Cartel operated in various countries from the 1970s to 1989.\n",
      "corr: yes, res: YES. The evaluated answer states that Jose Gonzalo Rodriguez Gacha and other leaders of the Medellin Cartel operated from the 1970s to 1989, which aligns with the timeframe of the 1970s and 1980s provided in the correct answer., f1: 0.1904761904761905, em: False\n",
      "\n",
      "Clement Greenberg ,  Agatha Christie died later.\n",
      "corr: no, res: NO\n",
      "\n",
      "Agatha Christie died in 1976, while Clement Greenberg died in 1994. Therefore, Clement Greenberg died later than Agatha Christie., f1: 0, em: False\n",
      "\n",
      "Battle of Britain and the Battle of Malta ,  The battles in the European theatre of World War II where Arthur Noss served as a gunner are not explicitly mentioned in the provided context.\n",
      "corr: no, res: NO\n",
      "\n",
      "The provided answer does not mention the specific battles, Battle of Britain and the Battle of Malta, where Arthur Noss served as a gunner. Therefore, it is not factually correct or complete based on the correct answer., f1: 0.07142857142857142, em: False\n",
      "\n",
      "Alopecurus ,  Alopecurus\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "A.P. M√∏ller ,  Peter M√¶rsk M√∏ller\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"Peter M√¶rsk M√∏ller\" is incorrect. M√¶rsk Mc-Kinney M√∏ller's father was A.P. M√∏ller, not Peter M√¶rsk M√∏ller., f1: 0.4, em: False\n",
      "\n",
      "Samantha Cristoforetti ,  Samantha Cristoforetti\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Park Ye-jin ,  Park Ye-jin\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Keelung ,  Keelung\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "located in Olathe, Kansas ,  Tyler Kalinoski attended Olathe East High School in Olathe, Kansas, United States.\n",
      "corr: yes, res: YES, f1: 0.375, em: False\n",
      "\n",
      "Drifting ,  Counter-steering or opposite lock technique\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"Counter-steering or opposite lock technique\" is not fully correct. While counter-steering is a technique used in drifting, the question specifically asks for the oversteering technique that D1NZ is based on, which is \"drifting.\" Counter-steering is a component of drifting, but it is not the complete answer., f1: 0, em: False\n",
      "\n",
      "Aloha  ªOe ,  Aloha  ªOe\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "approximately 3 mi ,  Approximately 3.5 miles\n",
      "corr: no, res: NO\n",
      "\n",
      "The provided answer states \"approximately 3.5 miles,\" which is not the same as the correct answer of \"approximately 3 mi.\" The difference in distance makes the answer factually incorrect., f1: 0.3333333333333333, em: False\n",
      "\n",
      "Steve Martin ,  Steve Martin\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "10 October 2010 ,  The Cura√ßao Centre for Correction and Detention changed its name to Sentro di Detenshon i Korekshon K√≤rsou on 10 October 2010.\n",
      "corr: yes, res: YES, f1: 0.2608695652173913, em: False\n",
      "\n",
      "Eastern Conference champion Orlando Magic against the Western Conference champion Houston Rockets. ,  The Orlando Magic and the Houston Rockets played against each other in the 1995 NBA Finals.\n",
      "corr: yes, res: YES, f1: 0.41666666666666663, em: False\n",
      "\n",
      "Ballarat Bitter is a 4.9% (abv) Australian beer ,  Ballarat Bitter\n",
      "corr: no, res: NO\n",
      "\n",
      "The provided answer \"Ballarat Bitter\" is incomplete. It does not mention that it is a British style pale ale or that its label features a character conceived in 1926, which are key elements of the question., f1: 0.4444444444444445, em: False\n",
      "\n",
      "Bengt Mikael Stanne ,  Mikael Stanne\n",
      "corr: yes, res: YES, f1: 0.8, em: False\n",
      "\n",
      "French ,  France\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "Evey's mother ,  Evey's mother\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "The Hindu Group ,  Frontline\n",
      "corr: no, res: NO\n",
      "\n",
      "Explanation: The answer \"Frontline\" is not fully correct because it refers to the magazine itself, not the company that publishes it. The correct answer should be \"The Hindu Group,\" which is the company that publishes Frontline magazine., f1: 0, em: False\n",
      "\n",
      "Turkey ,  Istanbul, Turkey\n",
      "corr: yes, res: YES\n",
      "\n",
      "The answer \"Istanbul, Turkey\" is factually correct and complete because both the Atik Valide Mosque and the Valens Aqueduct are located in Istanbul, which is a city in Turkey. Therefore, the answer includes the correct country, Turkey., f1: 0.6666666666666666, em: False\n",
      "\n",
      "no ,  Helen Dunmore is not of West Indian descent, while M. P. Shiel is of West Indian descent.\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "Fleetwood Mac ,  Fleetwood Mac\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "almost 3 million ,  The population of Mirdit√´ municipality in Albania in 2016 was almost 3 million people.\n",
      "corr: no, res: NO\n",
      "\n",
      "The evaluated answer incorrectly states that the population of the Mirdit√´ municipality itself was almost 3 million people, whereas the correct answer refers to the population of the entire country of Albania in 2016., f1: 0.375, em: False\n",
      "\n",
      "LaLee's Kin: The Legacy of Cotton ,  Gimme Shelter\n",
      "corr: no, res: NO, f1: 0, em: False\n",
      "\n",
      "Kingdom of the Isles ,  Kingdom of the Isles\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Part I ,  Part I of Handel's Messiah covers the birth of a child born in Bethlehem according to the gospels of Luke and Matthew.\n",
      "corr: yes, res: YES, f1: 0.1904761904761905, em: False\n",
      "\n",
      "In a Better World ,  In a Better World\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "child actor ,  Former child actor\n",
      "corr: yes, res: YES, f1: 0.8, em: False\n",
      "\n",
      "Joshua Rowley ,  Claude de Forbin\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer provided, \"Claude de Forbin,\" is incorrect. The correct answer is \"Joshua Rowley.\" Claude de Forbin was a French naval officer, not the commander who captured French ships., f1: 0, em: False\n",
      "\n",
      "New York City ,  New York City\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "rock and roll ,  Rhythm and Blues, Rock and Roll\n",
      "corr: no, res: NO\n",
      "\n",
      "The provided answer includes \"Rhythm and Blues\" in addition to \"Rock and Roll.\" While \"Rock and Roll\" is correct, the inclusion of \"Rhythm and Blues\" makes the answer not fully correct based on the question, which asks for a common genre both artists played. The correct answer should only be \"Rock and Roll.\", f1: 0.6666666666666666, em: False\n",
      "\n",
      "China ,  Sanming is in western Fujian province, China, and Jiutai District is under Changchun, Jilin province, China.\n",
      "corr: yes, res: YES, f1: 0.11764705882352941, em: False\n",
      "\n",
      "The Los Angeles Dance Theater ,  The American Ballet\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"The American Ballet\" is not correct. The correct answer is \"The Los Angeles Dance Theater,\" which was founded by George Balanchine to create a live stage version of the 1942 film Casablanca., f1: 0, em: False\n",
      "\n",
      "Lostprophets disbanded in 2013 after Watkins was charged with sexual offences in late 2012. ,  Ian Watkins\n",
      "corr: yes, res: YES, f1: 0.125, em: False\n",
      "\n",
      "Don Jeffrey \"Jeff\" Meldrum (born May 24, 1958) is a Professor of Anatomy and Anthropology ,  Jeff Meldrum\n",
      "corr: yes, res: YES, f1: 0.25, em: False\n",
      "\n",
      "Charles and Thomas Guard ,  The Guard Brothers, Charles and Thomas Guard\n",
      "corr: yes, res: YES, f1: 0.8, em: False\n",
      "\n",
      "Wheeling, West Virginia ,  The first major improved highway is the National Road in the United States, built between 1811 and 1837 by the federal government.\n",
      "corr: no, res: NO\n",
      "\n",
      "The evaluated answer provides information about the National Road, which is related to the first major improved highway, but it does not specifically mention Wheeling, West Virginia, as the location. Therefore, it is not fully correct based on the given correct answer., f1: 0, em: False\n",
      "\n",
      "Godiva ,  Godiva\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Eric Banadinoviƒá ,  John Walker\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer \"John Walker\" is factually incorrect. The correct answer is Eric Banadinoviƒá, who is known professionally as Eric Bana. He began his career in the sketch comedy series \"Full Frontal.\", f1: 0, em: False\n",
      "\n",
      "Germany ,  Both L√∂wenbr√§u and B√ºrgerbr√§ukeller are located in Munich, Germany.\n",
      "corr: yes, res: YES, f1: 0.19999999999999998, em: False\n",
      "\n",
      "1998 ,  1978\n",
      "corr: no, res: NO\n",
      "\n",
      "The satellite that Progress MS-09 was used to resupply is the International Space Station (ISS), which first launched into orbit in 1998. The provided answer of 1978 is incorrect., f1: 0, em: False\n",
      "\n",
      "Chippewa County ,  Manistee County, Michigan\n",
      "corr: no, res: NO\n",
      "\n",
      "Explanation: The provided answer states \"Manistee County, Michigan,\" which is incorrect. The correct answer is \"Chippewa County.\" H-63 runs near the county seat of Chippewa County, not Manistee County., f1: 0.4, em: False\n",
      "\n",
      "IRA ,  The Barrack buster used to shoot down the British Army Lynx helicopter was fired by a unit of the IRA's South Armagh Brigade.\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "Lincoln Memorial University ,  Lincoln Memorial University\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Hanako Muraoka ,  Hanako Muraoka\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "no ,  Yasuzo Masumura is Japanese and a film director, while Pitof is French and a visual effects supervisor and director.\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "Avengers Assemble ,  Avengers Assemble\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Pieter van Musschenbroek ,  Pieter van Musschenbroek\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Dirt track racing ,  Dirt track racing\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Fort Worth ,  Fort Worth\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Blood and soil ,  The slogan that came about from the idea of Lebensraum and was supported by the Nazi Party until the end of World War II was 'Blood and Soil'.\n",
      "corr: yes, res: YES, f1: 0.2222222222222222, em: False\n",
      "\n",
      "2016 Oklahoma Sooners football team ,  The 2016 Oklahoma Sooners football team coached by Bob Stoops beat the 2016 Auburn Tigers football team.\n",
      "corr: yes, res: YES, f1: 0.5, em: False\n",
      "\n",
      "5,922 ,  The population of the city named after the vice president in April 1813, according to the 2010 census, is 5,922 in Elbridge, New York.\n",
      "corr: yes, res: YES, f1: 0.09523809523809523, em: False\n",
      "\n",
      "Take It Easy ,  Take It Easy\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "from around 1520 ,  The oldest riverside tavern in Wapping, the Prospect of Whitby, was established around 1520.\n",
      "corr: yes, res: YES, f1: 0.26666666666666666, em: False\n",
      "\n",
      "Paola Su√°rez ,  Paola Su√°rez\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "yes ,  Yes, both Joe Orton and Bernard-Marie Kolt√®s are playwrights.\n",
      "corr: yes, res: YES, f1: 0.19999999999999998, em: False\n",
      "\n",
      "Mark O'Connor ,  Mark O'Connor\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Philip Jos√© Farmer ,  Philip Jos√© Farmer lived longer than Bernard-Marie Kolt√®s.\n",
      "corr: yes, res: YES, f1: 0.5454545454545454, em: False\n",
      "\n",
      "Rockstar San Diego ,  Rockstar San Diego\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "no ,  Neither The Pogues nor Pillar are electronic dance music groups.\n",
      "corr: yes, res: YES, f1: 0, em: False\n",
      "\n",
      "New York University School of Law ,  New York University School of Law\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Bruce Conner ,  Bruce Conner\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Old School Reference and Index Compilation ,  OSRIC\n",
      "corr: yes, res: YES\n",
      "\n",
      "The answer \"OSRIC\" is an abbreviation for \"Old School Reference and Index Compilation,\" which matches the correct answer. Therefore, the provided answer is factually correct and complete., f1: 0, em: False\n",
      "\n",
      "Jeffrey Adam \"Duff\" Goldman ,  Duff Goldman\n",
      "corr: yes, res: YES, f1: 0.6666666666666666, em: False\n",
      "\n",
      "Tai Frasier in \"Clueless\" ,  Cuba Gooding Jr. with breakthrough role as Tre Styles in \"Boyz n the Hood\" and a native of Atlanta\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer provided is incorrect. The question asks for the breakthrough role of an actor starring in \"Good Boy!\" who is a native of Atlanta. The correct answer is Tai Frasier in \"Clueless,\" referring to Brittany Murphy, who starred in \"Good Boy!\" and was born in Atlanta. The provided answer incorrectly mentions Cuba Gooding Jr. and his role in \"Boyz n the Hood,\" which is unrelated to the question., f1: 0.09523809523809523, em: False\n",
      "\n",
      "Harry Lillis \"Bing\" Crosby Jr. ,  Bing Crosby\n",
      "corr: yes, res: YES, f1: 0.5714285714285715, em: False\n",
      "\n",
      "Greyia ,  Greyia genus contains more species than Calibanus genus.\n",
      "corr: yes, res: YES, f1: 0.2222222222222222, em: False\n",
      "\n",
      "Jaguar Land Rover ,  Jaguar Land Rover\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Ken Howard ,  Phelan Beale\n",
      "corr: no, res: NO\n",
      "\n",
      "Phelan Beale is a character in \"Grey Gardens,\" not the actor who played the American attorney. The correct actor is Ken Howard., f1: 0, em: False\n",
      "\n",
      "after ,  Princess Charlotte of Cambridge was born after the repealing of the Royal Marriages Act 1772\n",
      "corr: yes, res: YES, f1: 0.14285714285714288, em: False\n",
      "\n",
      "1918 ,  1918\n",
      "corr: yes, res: YES, f1: 1.0, em: True\n",
      "\n",
      "Jean Erdman ,  Jean Erdman, Christoph Marthaler, Fotos Politis, Richard Foreman\n",
      "corr: no, res: NO\n",
      "\n",
      "The answer lists multiple names, but only Jean Erdman is correct. The inclusion of Christoph Marthaler, Fotos Politis, and Richard Foreman makes the answer factually incorrect and incomplete., f1: 0.4, em: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the EM and F1 scores are good to use\n",
    "for i in range(iteration):\n",
    "    print(states['gt_answer'][i], ', ', states['answer'][i])\n",
    "    corr = results['correctness'][i]\n",
    "    res = results['response'][i]\n",
    "    f1score = results['f1'][i]\n",
    "    em = results['exact_match'][i]\n",
    "    print(f'corr: {corr}, res: {res}, f1: {f1score}, em: {em}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from prompt import PromptTemplate\n",
    "from utils import load_llm\n",
    "\n",
    "class SingleRes(BaseModel):\n",
    "    answer: str\n",
    "    reasoning: str\n",
    "    supporting_documents: list[int]\n",
    "\n",
    "# Prompt\n",
    "single_prompt = [\n",
    "    SystemMessage(\n",
    "        PromptTemplate.direct_solution.format(\n",
    "            Problem = question,\n",
    "            Context = contexts\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# Load LLM\n",
    "llm = load_llm(config.model)\n",
    "structured_llm = llm.with_structured_output(SingleRes)\n",
    "\n",
    "response = structured_llm.invoke(single_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal-directedness-7eAwMRxE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
