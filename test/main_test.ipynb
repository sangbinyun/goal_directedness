{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load .env\n",
    "load_dotenv('/home/sangbin_yun/dev/goal_directedness/')\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\"hotpot_qa\", 'distractor', split='train', trust_remote_code=True)\n",
    "dev_set = load_dataset(\"hotpot_qa\", 'distractor', split='validation', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]['context']['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "glove_word_file = \"glove.840B.300d.txt\"\n",
    "\n",
    "word_emb_file = \"word_emb.json\"\n",
    "char_emb_file = \"char_emb.json\"\n",
    "train_eval = \"train_eval.json\"\n",
    "dev_eval = \"dev_eval.json\"\n",
    "test_eval = \"test_eval.json\"\n",
    "word2idx_file = \"word2idx.json\"\n",
    "char2idx_file = \"char2idx.json\"\n",
    "idx2word_file = 'idx2word.json'\n",
    "idx2char_file = 'idx2char.json'\n",
    "train_record_file = 'train_record.pkl'\n",
    "dev_record_file = 'dev_record.pkl'\n",
    "test_record_file = 'test_record.pkl'\n",
    "\n",
    "\n",
    "parser.add_argument('--mode', type=str, default='train')\n",
    "parser.add_argument('--data_file', type=str)\n",
    "parser.add_argument('--glove_word_file', type=str, default=glove_word_file)\n",
    "parser.add_argument('--save', type=str, default='HOTPOT')\n",
    "\n",
    "parser.add_argument('--word_emb_file', type=str, default=word_emb_file)\n",
    "parser.add_argument('--char_emb_file', type=str, default=char_emb_file)\n",
    "parser.add_argument('--train_eval_file', type=str, default=train_eval)\n",
    "parser.add_argument('--dev_eval_file', type=str, default=dev_eval)\n",
    "parser.add_argument('--test_eval_file', type=str, default=test_eval)\n",
    "parser.add_argument('--word2idx_file', type=str, default=word2idx_file)\n",
    "parser.add_argument('--char2idx_file', type=str, default=char2idx_file)\n",
    "parser.add_argument('--idx2word_file', type=str, default=idx2word_file)\n",
    "parser.add_argument('--idx2char_file', type=str, default=idx2char_file)\n",
    "\n",
    "parser.add_argument('--train_record_file', type=str, default=train_record_file)\n",
    "parser.add_argument('--dev_record_file', type=str, default=dev_record_file)\n",
    "parser.add_argument('--test_record_file', type=str, default=test_record_file)\n",
    "\n",
    "parser.add_argument('--glove_char_size', type=int, default=94)\n",
    "parser.add_argument('--glove_word_size', type=int, default=int(2.2e6))\n",
    "parser.add_argument('--glove_dim', type=int, default=300)\n",
    "parser.add_argument('--char_dim', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--para_limit', type=int, default=1000)\n",
    "parser.add_argument('--ques_limit', type=int, default=80)\n",
    "parser.add_argument('--sent_limit', type=int, default=100)\n",
    "parser.add_argument('--char_limit', type=int, default=16)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--checkpoint', type=int, default=1000)\n",
    "parser.add_argument('--period', type=int, default=100)\n",
    "parser.add_argument('--init_lr', type=float, default=0.5)\n",
    "parser.add_argument('--keep_prob', type=float, default=0.8)\n",
    "parser.add_argument('--hidden', type=int, default=80)\n",
    "parser.add_argument('--char_hidden', type=int, default=100)\n",
    "parser.add_argument('--patience', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=13)\n",
    "\n",
    "parser.add_argument('--sp_lambda', type=float, default=0.0)\n",
    "\n",
    "parser.add_argument('--data_split', type=str, default='train')\n",
    "parser.add_argument('--fullwiki', action='store_true')\n",
    "parser.add_argument('--prediction_file', type=str)\n",
    "parser.add_argument('--sp_threshold', type=float, default=0.3)\n",
    "\n",
    "config = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_source, model, eval_file, config, prediction_file):\n",
    "    answer_dict = {}\n",
    "    sp_dict = {}\n",
    "    sp_th = config.sp_threshold\n",
    "    for step, data in enumerate(tqdm(data_source)):\n",
    "        '''\n",
    "        Dataset load\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        Model prediction\n",
    "        answer_dict_ = \n",
    "        answer_dict.update(answer_dict_)\n",
    "        '''\n",
    "\n",
    "        predict_support_np = torch.sigmoid(predict_support[:, :, 1]).data.cpu().numpy()\n",
    "        for i in range(predict_support_np.shape[0]):\n",
    "            cur_sp_pred = []\n",
    "            cur_id = data['ids'][i]\n",
    "            for j in range(predict_support_np.shape[1]):\n",
    "                if j >= len(eval_file[cur_id]['sent2title_ids']): break\n",
    "                if predict_support_np[i, j] > sp_th:\n",
    "                    cur_sp_pred.append(eval_file[cur_id]['sent2title_ids'][j])\n",
    "            sp_dict.update({cur_id: cur_sp_pred})\n",
    "\n",
    "    prediction = {'answer': answer_dict, 'sp': sp_dict}\n",
    "    with open(prediction_file, 'w') as f:\n",
    "        json.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ujson as json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def update_answer(metrics, prediction, gold):\n",
    "    em = exact_match_score(prediction, gold)\n",
    "    f1, prec, recall = f1_score(prediction, gold)\n",
    "    metrics['em'] += float(em)\n",
    "    metrics['f1'] += f1\n",
    "    metrics['prec'] += prec\n",
    "    metrics['recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def update_sp(metrics, prediction, gold):\n",
    "    cur_sp_pred = set(map(tuple, prediction))\n",
    "    gold_sp_pred = set(map(tuple, gold))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    metrics['sp_em'] += em\n",
    "    metrics['sp_f1'] += f1\n",
    "    metrics['sp_prec'] += prec\n",
    "    metrics['sp_recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def eval(prediction_file, gold_file):\n",
    "    with open(prediction_file) as f:\n",
    "        prediction = json.load(f)\n",
    "    with open(gold_file) as f:\n",
    "        gold = json.load(f)\n",
    "\n",
    "    metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0,\n",
    "        'sp_em': 0, 'sp_f1': 0, 'sp_prec': 0, 'sp_recall': 0,\n",
    "        'joint_em': 0, 'joint_f1': 0, 'joint_prec': 0, 'joint_recall': 0}\n",
    "    for dp in gold:\n",
    "        cur_id = dp['_id']\n",
    "        can_eval_joint = True\n",
    "        if cur_id not in prediction['answer']:\n",
    "            print('missing answer {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            em, prec, recall = update_answer(\n",
    "                metrics, prediction['answer'][cur_id], dp['answer'])\n",
    "        if cur_id not in prediction['sp']:\n",
    "            print('missing sp fact {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            sp_em, sp_prec, sp_recall = update_sp(\n",
    "                metrics, prediction['sp'][cur_id], dp['supporting_facts'])\n",
    "\n",
    "        if can_eval_joint:\n",
    "            joint_prec = prec * sp_prec\n",
    "            joint_recall = recall * sp_recall\n",
    "            if joint_prec + joint_recall > 0:\n",
    "                joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "            else:\n",
    "                joint_f1 = 0.\n",
    "            joint_em = em * sp_em\n",
    "\n",
    "            metrics['joint_em'] += joint_em\n",
    "            metrics['joint_f1'] += joint_f1\n",
    "            metrics['joint_prec'] += joint_prec\n",
    "            metrics['joint_recall'] += joint_recall\n",
    "\n",
    "    N = len(gold)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eval(sys.argv[1], sys.argv[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import dspy\n",
    "from dspy.predict.chain_of_thought import ChainOfThought\n",
    "from react.agent import ReasoningAgent\n",
    "import ray  # For parallelization of subtasks\n",
    "import torch  # PyTorch for model training and task handling\n",
    "from langgraph import LangGraph, Node, Edge  # Importing LangGraph library\n",
    "\n",
    "\n",
    "# Horizontal Component: Define a sequential, goal-directed task solver\n",
    "class HorizontalTaskSolver:\n",
    "    def __init__(self, task_sequence):\n",
    "        self.task_sequence = task_sequence  # List of tasks to be solved step-by-step\n",
    "\n",
    "    def solve(self):\n",
    "        solution = []\n",
    "        for task in self.task_sequence:\n",
    "            # Use a chain of thought to reason through each task\n",
    "            rationale, result = self.reason_task(task)\n",
    "            solution.append({\"task\": task, \"rationale\": rationale, \"result\": result})\n",
    "        return solution\n",
    "\n",
    "    def reason_task(self, task):\n",
    "        # Chain of Thought style reasoning: explain each step\n",
    "        cot = ChainOfThought(task)\n",
    "        rationale = cot.generate_rationale()  # Generate reasoning for this task\n",
    "        result = cot.predict()  # Predict the outcome\n",
    "        return rationale, result\n",
    "\n",
    "\n",
    "# Vertical Component: Define a parallel subtask solver\n",
    "@ray.remote  # Distribute subtasks in parallel\n",
    "class VerticalTaskSolver:\n",
    "    def __init__(self, subtasks):\n",
    "        self.subtasks = subtasks\n",
    "\n",
    "    def solve(self):\n",
    "        results = []\n",
    "        for subtask in self.subtasks:\n",
    "            try:\n",
    "                result = self.process_subtask(subtask)\n",
    "                results.append({\"subtask\": subtask, \"result\": result, \"status\": \"success\"})\n",
    "            except Exception as e:\n",
    "                results.append({\"subtask\": subtask, \"result\": None, \"status\": \"failed\", \"error\": str(e)})\n",
    "        return results\n",
    "\n",
    "    def process_subtask(self, subtask):\n",
    "        # Process each subtask independently (e.g., classification, data filtering)\n",
    "        # Placeholder for task-specific processing logic\n",
    "        result = ReasoningAgent(subtask).act()  # Reason and act on the subtask\n",
    "        return result\n",
    "\n",
    "# Divide the user prompt into horizontal and vertical tasks\n",
    "class TaskDivider(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.divider = dspy.ChainOfThought(\"Analyze the given prompt and divide it into sequential tasks and parallel subtasks.\")\n",
    "\n",
    "    def forward(self, prompt: str) -> Tuple[List[str], List[str]]:\n",
    "        analysis = self.divider(prompt=prompt)\n",
    "        horizontal_tasks = analysis.horizontal_tasks\n",
    "        vertical_subtasks = analysis.vertical_subtasks\n",
    "        return horizontal_tasks, vertical_subtasks\n",
    "\n",
    "# Orchestrate Horizontal and Vertical Processing\n",
    "class TaskOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.task_divider = TaskDivider()\n",
    "        self.graph = LangGraph()  # Initialize LangGraph\n",
    "\n",
    "    def divide_and_create_graph(self, user_prompt):\n",
    "        # Step 1: Divide tasks using TaskDivider\n",
    "        horizontal_tasks, vertical_subtasks = self.task_divider(user_prompt)\n",
    "\n",
    "        # Step 2: Build LangGraph nodes and edges\n",
    "        horizontal_solver = HorizontalTaskSolver(horizontal_tasks)\n",
    "        vertical_solver = VerticalTaskSolver.remote(vertical_subtasks)\n",
    "\n",
    "        # Create nodes for LangGraph\n",
    "        horizontal_node = Node(name=\"Horizontal Task Solver\", task=horizontal_solver.solve)\n",
    "        vertical_node = Node(name=\"Vertical Task Solver\", task=vertical_solver.solve.remote)\n",
    "\n",
    "        # Add nodes to graph\n",
    "        self.graph.add_node(horizontal_node)\n",
    "        self.graph.add_node(vertical_node)\n",
    "\n",
    "        # Create edge to connect horizontal and vertical tasks (if needed)\n",
    "        # For instance, if vertical tasks depend on results of horizontal tasks\n",
    "        self.graph.add_edge(Edge(from_node=horizontal_node, to_node=vertical_node))\n",
    "\n",
    "    def divide_tasks(self, user_prompt: str) -> Tuple[List[str], List[str]]:\n",
    "        return self.task_divider(user_prompt)\n",
    "\n",
    "    def orchestrate(self):\n",
    "        # Solve horizontal tasks sequentially\n",
    "        horizontal_results = self.horizontal_solver.solve()\n",
    "        \n",
    "        # Solve vertical tasks in parallel\n",
    "        vertical_results = ray.get(self.vertical_solver.solve.remote())\n",
    "        \n",
    "        return {\"horizontal_results\": horizontal_results, \"vertical_results\": vertical_results}\n",
    "\n",
    "# 4. Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_prompt = \"Example user prompt to divide into tasks\"\n",
    "    \n",
    "    orchestrator = TaskOrchestrator()\n",
    "    results = orchestrator.divide_and_orchestrate(user_prompt)\n",
    "\n",
    "    print(\"Horizontal Task Results:\", results[\"horizontal_results\"])\n",
    "    print(\"Vertical Task Results:\", results[\"vertical_results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langgraph import LangGraph, Node, Edge  # Importing LangGraph library\n",
    "from dspy.predict.chain_of_thought import ChainOfThought\n",
    "from react.agent import ReasoningAgent\n",
    "import ray  # For parallelization\n",
    "import torch  # PyTorch for model handling\n",
    "\n",
    "# Define the prompts for each node to guide the model\n",
    "PROMPTS = {\n",
    "    \"problem_verification\": \"Verify the problem based on the user-prompt, validate clarity, and redefine if needed.\",\n",
    "    \"problem_decomposition\": \"Decompose the problem into sequential and parallel tasks. Define the sequence and dependencies.\",\n",
    "    \"solve_sequential\": \"Solve each sequential subtask in the order given, considering dependencies.\",\n",
    "    \"combine_evidence\": \"Combine the solutions from each subtask to build a cohesive argument or evidence set.\",\n",
    "    \"provide_answer\": \"Based on the combined evidence, generate a final answer for the user.\"\n",
    "}\n",
    "\n",
    "# Node 1: Problem Verification\n",
    "class ProblemVerificationNode:\n",
    "    def __init__(self, prompt=PROMPTS[\"problem_verification\"]):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def verify_problem(self, user_prompt):\n",
    "        # Use ChainOfThought for validation\n",
    "        cot = ChainOfThought(self.prompt)\n",
    "        validation_result = cot.predict(user_prompt)\n",
    "        return validation_result\n",
    "\n",
    "# Node 2: Problem Decomposition\n",
    "class ProblemDecompositionNode:\n",
    "    def __init__(self, prompt=PROMPTS[\"problem_decomposition\"]):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def decompose_problem(self, user_prompt):\n",
    "        cot = ChainOfThought(self.prompt)\n",
    "        decomposition_result = cot.predict(user_prompt)\n",
    "        # Expected output is in format (horizontal_tasks, vertical_subtasks)\n",
    "        return decomposition_result.horizontal_tasks, decomposition_result.vertical_subtasks\n",
    "\n",
    "# Node 3: Sequential Subtask Solver\n",
    "class SequentialSubtaskSolverNode:\n",
    "    def __init__(self, prompt=PROMPTS[\"solve_sequential\"]):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def solve_sequentially(self, horizontal_tasks):\n",
    "        solutions = []\n",
    "        for task in horizontal_tasks:\n",
    "            cot = ChainOfThought(self.prompt)\n",
    "            solution = cot.predict(task)\n",
    "            solutions.append(solution)\n",
    "        return solutions\n",
    "\n",
    "# Node 4: Evidence Combination\n",
    "class EvidenceCombinationNode:\n",
    "    def __init__(self, prompt=PROMPTS[\"combine_evidence\"]):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def combine_evidence(self, solutions):\n",
    "        cot = ChainOfThought(self.prompt)\n",
    "        combined_evidence = cot.predict(solutions)\n",
    "        return combined_evidence\n",
    "\n",
    "# Node 5: Answer Generation\n",
    "class AnswerGenerationNode:\n",
    "    def __init__(self, prompt=PROMPTS[\"provide_answer\"]):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def generate_answer(self, combined_evidence):\n",
    "        cot = ChainOfThought(self.prompt)\n",
    "        final_answer = cot.predict(combined_evidence)\n",
    "        return final_answer\n",
    "\n",
    "# Define the LangGraph and connect the nodes\n",
    "class ProblemSolvingGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = LangGraph()\n",
    "\n",
    "        # Initialize nodes\n",
    "        self.problem_verification_node = Node(name=\"Problem Verification\", task=self.problem_verification_task)\n",
    "        self.problem_decomposition_node = Node(name=\"Problem Decomposition\", task=self.problem_decomposition_task)\n",
    "        self.sequential_solver_node = Node(name=\"Sequential Subtask Solver\", task=self.sequential_solver_task)\n",
    "        self.evidence_combination_node = Node(name=\"Evidence Combination\", task=self.evidence_combination_task)\n",
    "        self.answer_generation_node = Node(name=\"Answer Generation\", task=self.answer_generation_task)\n",
    "\n",
    "        # Add nodes to graph\n",
    "        self.graph.add_node(self.problem_verification_node)\n",
    "        self.graph.add_node(self.problem_decomposition_node)\n",
    "        self.graph.add_node(self.sequential_solver_node)\n",
    "        self.graph.add_node(self.evidence_combination_node)\n",
    "        self.graph.add_node(self.answer_generation_node)\n",
    "\n",
    "        # Define edges between nodes to enforce order\n",
    "        self.graph.add_edge(Edge(from_node=self.problem_verification_node, to_node=self.problem_decomposition_node))\n",
    "        self.graph.add_edge(Edge(from_node=self.problem_decomposition_node, to_node=self.sequential_solver_node))\n",
    "        self.graph.add_edge(Edge(from_node=self.sequential_solver_node, to_node=self.evidence_combination_node))\n",
    "        self.graph.add_edge(Edge(from_node=self.evidence_combination_node, to_node=self.answer_generation_node))\n",
    "\n",
    "    # Define each node task\n",
    "    def problem_verification_task(self, user_prompt):\n",
    "        verifier = ProblemVerificationNode()\n",
    "        return verifier.verify_problem(user_prompt)\n",
    "\n",
    "    def problem_decomposition_task(self, user_prompt):\n",
    "        decomposer = ProblemDecompositionNode()\n",
    "        return decomposer.decompose_problem(user_prompt)\n",
    "\n",
    "    def sequential_solver_task(self, horizontal_tasks):\n",
    "        solver = SequentialSubtaskSolverNode()\n",
    "        return solver.solve_sequentially(horizontal_tasks)\n",
    "\n",
    "    def evidence_combination_task(self, solutions):\n",
    "        combiner = EvidenceCombinationNode()\n",
    "        return combiner.combine_evidence(solutions)\n",
    "\n",
    "    def answer_generation_task(self, combined_evidence):\n",
    "        answerer = AnswerGenerationNode()\n",
    "        return answerer.generate_answer(combined_evidence)\n",
    "\n",
    "    # Execute the graph\n",
    "    def solve_problem(self, user_prompt):\n",
    "        # Execute the graph from start to end\n",
    "        self.graph.execute({\"user_prompt\": user_prompt})\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_prompt = \"Provide a solution for a multi-step problem.\"\n",
    "\n",
    "    problem_solver_graph = ProblemSolvingGraph()\n",
    "    results = problem_solver_graph.solve_problem(user_prompt)\n",
    "\n",
    "    print(\"Final Answer:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal-directedness-7eAwMRxE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
