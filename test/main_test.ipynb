{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env('LANGCHAIN_TRACING_V2')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\"hotpot_qa\", 'distractor', split='train', trust_remote_code=True)\n",
    "dev_set = load_dataset(\"hotpot_qa\", 'distractor', split='validation', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "glove_word_file = \"glove.840B.300d.txt\"\n",
    "\n",
    "word_emb_file = \"word_emb.json\"\n",
    "char_emb_file = \"char_emb.json\"\n",
    "train_eval = \"train_eval.json\"\n",
    "dev_eval = \"dev_eval.json\"\n",
    "test_eval = \"test_eval.json\"\n",
    "word2idx_file = \"word2idx.json\"\n",
    "char2idx_file = \"char2idx.json\"\n",
    "idx2word_file = 'idx2word.json'\n",
    "idx2char_file = 'idx2char.json'\n",
    "train_record_file = 'train_record.pkl'\n",
    "dev_record_file = 'dev_record.pkl'\n",
    "test_record_file = 'test_record.pkl'\n",
    "\n",
    "\n",
    "parser.add_argument('--mode', type=str, default='train')\n",
    "parser.add_argument('--data_file', type=str)\n",
    "parser.add_argument('--glove_word_file', type=str, default=glove_word_file)\n",
    "parser.add_argument('--save', type=str, default='HOTPOT')\n",
    "\n",
    "parser.add_argument('--word_emb_file', type=str, default=word_emb_file)\n",
    "parser.add_argument('--char_emb_file', type=str, default=char_emb_file)\n",
    "parser.add_argument('--train_eval_file', type=str, default=train_eval)\n",
    "parser.add_argument('--dev_eval_file', type=str, default=dev_eval)\n",
    "parser.add_argument('--test_eval_file', type=str, default=test_eval)\n",
    "parser.add_argument('--word2idx_file', type=str, default=word2idx_file)\n",
    "parser.add_argument('--char2idx_file', type=str, default=char2idx_file)\n",
    "parser.add_argument('--idx2word_file', type=str, default=idx2word_file)\n",
    "parser.add_argument('--idx2char_file', type=str, default=idx2char_file)\n",
    "\n",
    "parser.add_argument('--train_record_file', type=str, default=train_record_file)\n",
    "parser.add_argument('--dev_record_file', type=str, default=dev_record_file)\n",
    "parser.add_argument('--test_record_file', type=str, default=test_record_file)\n",
    "\n",
    "parser.add_argument('--glove_char_size', type=int, default=94)\n",
    "parser.add_argument('--glove_word_size', type=int, default=int(2.2e6))\n",
    "parser.add_argument('--glove_dim', type=int, default=300)\n",
    "parser.add_argument('--char_dim', type=int, default=8)\n",
    "\n",
    "parser.add_argument('--para_limit', type=int, default=1000)\n",
    "parser.add_argument('--ques_limit', type=int, default=80)\n",
    "parser.add_argument('--sent_limit', type=int, default=100)\n",
    "parser.add_argument('--char_limit', type=int, default=16)\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--checkpoint', type=int, default=1000)\n",
    "parser.add_argument('--period', type=int, default=100)\n",
    "parser.add_argument('--init_lr', type=float, default=0.5)\n",
    "parser.add_argument('--keep_prob', type=float, default=0.8)\n",
    "parser.add_argument('--hidden', type=int, default=80)\n",
    "parser.add_argument('--char_hidden', type=int, default=100)\n",
    "parser.add_argument('--patience', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=13)\n",
    "\n",
    "parser.add_argument('--sp_lambda', type=float, default=0.0)\n",
    "\n",
    "parser.add_argument('--data_split', type=str, default='train')\n",
    "parser.add_argument('--fullwiki', action='store_true')\n",
    "parser.add_argument('--prediction_file', type=str)\n",
    "parser.add_argument('--sp_threshold', type=float, default=0.3)\n",
    "\n",
    "config = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_source, model, eval_file, config, prediction_file):\n",
    "    answer_dict = {}\n",
    "    sp_dict = {}\n",
    "    sp_th = config.sp_threshold\n",
    "    for step, data in enumerate(tqdm(data_source)):\n",
    "        '''\n",
    "        Dataset load\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        Model prediction\n",
    "        answer_dict_ = \n",
    "        answer_dict.update(answer_dict_)\n",
    "        '''\n",
    "\n",
    "        predict_support_np = torch.sigmoid(predict_support[:, :, 1]).data.cpu().numpy()\n",
    "        for i in range(predict_support_np.shape[0]):\n",
    "            cur_sp_pred = []\n",
    "            cur_id = data['ids'][i]\n",
    "            for j in range(predict_support_np.shape[1]):\n",
    "                if j >= len(eval_file[cur_id]['sent2title_ids']): break\n",
    "                if predict_support_np[i, j] > sp_th:\n",
    "                    cur_sp_pred.append(eval_file[cur_id]['sent2title_ids'][j])\n",
    "            sp_dict.update({cur_id: cur_sp_pred})\n",
    "\n",
    "    prediction = {'answer': answer_dict, 'sp': sp_dict}\n",
    "    with open(prediction_file, 'w') as f:\n",
    "        json.dump(prediction, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(metrics)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28meval\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m], \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ujson as json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def update_answer(metrics, prediction, gold):\n",
    "    em = exact_match_score(prediction, gold)\n",
    "    f1, prec, recall = f1_score(prediction, gold)\n",
    "    metrics['em'] += float(em)\n",
    "    metrics['f1'] += f1\n",
    "    metrics['prec'] += prec\n",
    "    metrics['recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def update_sp(metrics, prediction, gold):\n",
    "    cur_sp_pred = set(map(tuple, prediction))\n",
    "    gold_sp_pred = set(map(tuple, gold))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    metrics['sp_em'] += em\n",
    "    metrics['sp_f1'] += f1\n",
    "    metrics['sp_prec'] += prec\n",
    "    metrics['sp_recall'] += recall\n",
    "    return em, prec, recall\n",
    "\n",
    "def eval(prediction_file, gold_file):\n",
    "    with open(prediction_file) as f:\n",
    "        prediction = json.load(f)\n",
    "    with open(gold_file) as f:\n",
    "        gold = json.load(f)\n",
    "\n",
    "    metrics = {'em': 0, 'f1': 0, 'prec': 0, 'recall': 0,\n",
    "        'sp_em': 0, 'sp_f1': 0, 'sp_prec': 0, 'sp_recall': 0,\n",
    "        'joint_em': 0, 'joint_f1': 0, 'joint_prec': 0, 'joint_recall': 0}\n",
    "    for dp in gold:\n",
    "        cur_id = dp['_id']\n",
    "        can_eval_joint = True\n",
    "        if cur_id not in prediction['answer']:\n",
    "            print('missing answer {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            em, prec, recall = update_answer(\n",
    "                metrics, prediction['answer'][cur_id], dp['answer'])\n",
    "        if cur_id not in prediction['sp']:\n",
    "            print('missing sp fact {}'.format(cur_id))\n",
    "            can_eval_joint = False\n",
    "        else:\n",
    "            sp_em, sp_prec, sp_recall = update_sp(\n",
    "                metrics, prediction['sp'][cur_id], dp['supporting_facts'])\n",
    "\n",
    "        if can_eval_joint:\n",
    "            joint_prec = prec * sp_prec\n",
    "            joint_recall = recall * sp_recall\n",
    "            if joint_prec + joint_recall > 0:\n",
    "                joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "            else:\n",
    "                joint_f1 = 0.\n",
    "            joint_em = em * sp_em\n",
    "\n",
    "            metrics['joint_em'] += joint_em\n",
    "            metrics['joint_f1'] += joint_f1\n",
    "            metrics['joint_prec'] += joint_prec\n",
    "            metrics['joint_recall'] += joint_recall\n",
    "\n",
    "    N = len(gold)\n",
    "    for k in metrics.keys():\n",
    "        metrics[k] /= N\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    eval(sys.argv[1], sys.argv[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util.py\n",
    "def load_llm(model: str):\n",
    "    if model == 'openai':\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model = \"gpt-3.5-turbo\", # \"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5\", \"gpt-3\", \"gpt-2\", \"gpt-1\"\n",
    "            temperature = 0\n",
    "        )\n",
    "    elif model == 'ollama':\n",
    "        from langchain_ollama import OllamaLLM\n",
    "        llm = OllamaLLM(\n",
    "            model = 'llama3.2:3b-instruct-fp16',\n",
    "            base_url = \"http://localhost:11434\",\n",
    "            temperature = 0,\n",
    "            verbose = True,\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "def return_possible_models():\n",
    "    import ollama\n",
    "\n",
    "    # Get the list of available models\n",
    "    models = [i['model'] for i in ollama.list()['models']]\n",
    "    print(models)\n",
    "\n",
    "def connect_to_sql_memory():\n",
    "    import sqlite3\n",
    "    from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "    db_path = \"state_db/test.db\"\n",
    "    conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "    # checkpointer \n",
    "    memory = SqliteSaver(conn)\n",
    "\n",
    "    return memory\n",
    "\n",
    "llm = load_llm('openai')\n",
    "memory = connect_to_sql_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "class PromptTemplate:\n",
    "    # verification = \"You are an expert problem analyzer.\\n\" \\\n",
    "    #     \"Your task is to verify and refine the given problem statement by:\\n\" \\\n",
    "    #     \"1. Identifying the core objective and constraints\\n\" \\\n",
    "    #     \"2. Checking if the problem is well-defined and has clear success criteria\\n\" \\\n",
    "    #     \"3. Reformulating the problem if needed to make it more precise and actionable\\n\" \\\n",
    "    #     \"4. Highlighting any assumptions or prerequisites\\n\\n\" \\\n",
    "    #     \"Given problem: {Problem}\\n\" \\\n",
    "    #     \"Provide your analysis and refined problem statement with the following problem.\"\n",
    "\n",
    "    # decomposition = \"You are an expert in hierarchical problem decomposition.\\n\" \\\n",
    "    #     \"Your task is to break down the given problem into a structured hierarchy of sub-problems by:\\n\" \\\n",
    "    #     \"0. Thinking step by step\\n\" \\\n",
    "    #     \"1. Identifying the main components that need to be solved\\n\" \\\n",
    "    #     \"2. Arranging sub-problems in a vertical sequence where later steps depend on earlier ones\\n\" \\\n",
    "    #     \"3. For each sub-problem, further decompose if it requires multiple steps\\n\" \\\n",
    "    #     \"4. Specifying dependencies between sub-problems\\n\" \\\n",
    "    #     \"5. Keep the decomposition compact - if the problem can be solved in a single step, list it as one sub-problem\\n\\n\" \\\n",
    "    #     \"Problem: {Problem}\\n\" \\\n",
    "    #     \"Provide a hierarchical breakdown showing the vertical dependencies between sub-problems.\"\n",
    "\n",
    "    problem_analysis = \"You are an expert problem analyzer and decomposer with access to relevant context.\\n\" \\\n",
    "        \"Your task is to verify, refine, and break down the given problem by following these steps:\\n\" \\\n",
    "        \"0. Think step by step through the entire analysis process\\n\" \\\n",
    "        \"1. Verify the problem:\\n\" \\\n",
    "        \"   - Identify the core objective and constraints\\n\" \\\n",
    "        \"   - Check if the problem is well-defined with clear success criteria\\n\" \\\n",
    "        \"   - Highlight key assumptions and prerequisites\\n\" \\\n",
    "        \"   - Reformulate if needed to make more precise and actionable\\n\" \\\n",
    "        \"2. Decompose the refined problem:\\n\" \\\n",
    "        \"   - Identify the main components that need to be solved\\n\" \\\n",
    "        \"   - Arrange sub-problems in a vertical sequence with dependencies\\n\" \\\n",
    "        \"   - Further decompose sub-problems that require multiple steps\\n\" \\\n",
    "        \"   - Specifying dependencies between sub-problems\\n\" \\\n",
    "        \"   - Keep decomposition compact for single-step problems\\n\" \\\n",
    "        \"3. Consider the provided context:\\n\" \\\n",
    "        \"   - Review any relevant documentation provided\\n\" \\\n",
    "        \"   - Reference specific components or patterns that could be reused\\n\\n\" \\\n",
    "        \"Given problem: {Problem}\\n\" \\\n",
    "        \"Context: {Context}\\n\" \\\n",
    "        \"First provide your analysis and refined problem statement, then give a hierarchical breakdown showing the vertical dependencies between sub-problems.\"\n",
    "\n",
    "    subproblem_solution = \"You are an expert problem solver focused on solving specific sub-problems with access to relevant context.\\n\" \\\n",
    "        \"Your task is to solve the given sub-problem by:\\n\" \\\n",
    "        \"0. Think step by step through the entire analysis process\\n\" \\\n",
    "        \"1. Understanding the specific scope and requirements of this sub-problem\\n\" \\\n",
    "        \"2. Reviewing provided context and existing implementations\\n\" \\\n",
    "        \"3. Applying relevant domain knowledge and techniques\\n\" \\\n",
    "        \"4. Providing a clear and detailed solution\\n\" \\\n",
    "        \"5. Explaining your reasoning process\\n\" \\\n",
    "        \"6. Ensuring the solution aligns with any dependencies or constraints\\n\" \\\n",
    "        \"7. Referencing specific patterns or components from context where applicable\\n\\n\" \\\n",
    "        \"Sub-problems: {SubProblem}\\n\" \\\n",
    "        \"Dependencies: {Dependencies}\\n\" \\\n",
    "        \"Context: {Context}\\n\" \\\n",
    "        \"Provide your detailed solution for this specific sub-problem.\"\n",
    "\n",
    "    solution_aggregation = \"You are an expert in synthesizing solutions with access to relevant context.\\n\" \\\n",
    "        \"Your task is to aggregate the solutions from all sub-problems by:\\n\" \\\n",
    "        \"0. Think step by step through the entire analysis process\\n\" \\\n",
    "        \"1. Reviewing all sub-problem solutions\\n\" \\\n",
    "        \"2. Ensuring consistency across solutions\\n\" \\\n",
    "        \"3. Resolving any conflicts or inconsistencies\\n\" \\\n",
    "        \"4. Combining the solutions in a coherent way\\n\" \\\n",
    "        \"5. Verifying the combined solution addresses the refined problem\\n\\n\" \\\n",
    "        \"6. Refering any relevant context provided if it's necessary\\n\\n\" \\\n",
    "        \"Refined Problem: {RefinedProblem}\\n\" \\\n",
    "        \"Sub-problem Solutions: {SubProblemSolutions}\\n\" \\\n",
    "        \"Context: {Context}\\n\" \\\n",
    "        \"Provide a comprehensive solution that addresses the original problem by synthesizing all sub-solutions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph structure\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# State\n",
    "\n",
    "class State(MessagesState):\n",
    "    context: list[str]\n",
    "    problem: str\n",
    "    sub_problems: str\n",
    "    dependencies: list\n",
    "    sub_problem_solutions: list\n",
    "    reasoning: list\n",
    "    def __init__(self, given_context):\n",
    "        # super().__init__(given_context)\n",
    "        self.context = given_context\n",
    "\n",
    "class Refined(BaseModel):\n",
    "    problem: str\n",
    "    sub_problems: str\n",
    "    dependencies: list[str]\n",
    "\n",
    "class Solved(BaseModel):\n",
    "    sub_problem_solutions: list[str]\n",
    "    reasoning: list[str]\n",
    "\n",
    "# Graph functions\n",
    "def problem_analyzer(state: State):\n",
    "    # Refinement prompt\n",
    "    _orig_problem = state['messages'][-1].content\n",
    "    _prompt = [\n",
    "        SystemMessage(\n",
    "            PromptTemplate.problem_analysis.format(\n",
    "                Problem = _orig_problem,\n",
    "                Context = state['context']\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Invoke the LLM\n",
    "    structured_llm = llm.with_structured_output(Refined)\n",
    "    _response = structured_llm.invoke(_prompt)\n",
    "\n",
    "    return {\n",
    "        'problem': _response.problem, \n",
    "        'sub_problems': _response.sub_problems,\n",
    "        'dependencies': _response.dependencies\n",
    "    }\n",
    "\n",
    "def subproblem_solver(state: State):\n",
    "    # sub-problem solver\n",
    "    _prompt = [\n",
    "        SystemMessage(\n",
    "            PromptTemplate.subproblem_solution.format(\n",
    "                SubProblem = state['sub_problems'],\n",
    "                Dependencies = state['dependencies'],\n",
    "                Context = state['context']\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Invoke the LLM\n",
    "    structured_llm = llm.with_structured_output(Solved)\n",
    "    _response = structured_llm.invoke(_prompt)\n",
    "    \n",
    "    return {\n",
    "        'sub_problem_solutions': _response.sub_problem_solutions,\n",
    "        'reasoning': _response.reasoning\n",
    "    }\n",
    "\n",
    "def solution_aggregator(state: State):\n",
    "    # solution aggregator\n",
    "    _prompt = [\n",
    "        SystemMessage(\n",
    "            PromptTemplate.solution_aggregation.format(\n",
    "                RefinedProblem = state['problem'],  \n",
    "                SubProblemSolutions = state['sub_problem_solutions'],\n",
    "                Context = state['context']\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Invoke the LLM\n",
    "    _response = llm.invoke(_prompt)\n",
    "    \n",
    "    return {'messages': _response}\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"analyzer\", problem_analyzer)\n",
    "graph.add_node(\"solver\", subproblem_solver)\n",
    "graph.add_node(\"aggregator\", solution_aggregator)\n",
    "\n",
    "# Add edges\n",
    "graph.add_edge(START, \"analyzer\")\n",
    "graph.add_edge(\"analyzer\", \"solver\")\n",
    "graph.add_edge(\"solver\", \"aggregator\")\n",
    "graph.add_edge(\"aggregator\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph.compile(checkpointer = memory)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_set[0]\n",
    "\n",
    "# Extract values\n",
    "id = data['id']\n",
    "question = data['question']\n",
    "answer = data['answer']\n",
    "context = [\"{}: {}\".format(title, \" \".join(sentences)) for title, sentences in zip(data['context']['title'], data['context']['sentences'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the graphs\n",
    "thread = {\"configurable\": {\"thread_id\": id}}\n",
    "prompt = {\n",
    "    'messages': question,\n",
    "    'context': context\n",
    "}\n",
    "response = graph.invoke(prompt, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goal-directedness-7eAwMRxE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
